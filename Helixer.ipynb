{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNh9/WkOzSxUJyF+Ry/haTw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JKourelis/Colab_Helixer/blob/main/Helixer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/weberlab-hhu/Helixer/main/img/helixer.svg\" height=\"200\" align=\"right\" style=\"height:240px\">\n",
        "\n",
        "# Helixer: Deep Learning Gene Annotation\n",
        "\n",
        "**Structural genome annotation using Deep Neural Networks and Hidden Markov Models**\n",
        "\n",
        "Helixer is a tool for _ab initio_ prediction of gene structure that directly provides primary gene models in GFF3 format. It utilizes Deep Neural Networks to identify which base pairs belong to UTR/CDS/Intron regions of genes, making it performant and applicable to a wide variety of genomes.\n",
        "\n",
        "**Key Features:**\n",
        "- **Four trained lineage models**: fungi, land_plant, vertebrate, and invertebrate\n",
        "- **Intelligent parameter scaling**: Auto-adjusts settings based on genome assembly quality\n",
        "- **High performance**: Optimized for GPU acceleration with realistic genome-scale datasets\n",
        "- **Direct output**: Produces GFF3 files ready for downstream analysis\n",
        "- **Effector annotation**: Enhanced sensitivity with adjustable peak thresholds\n",
        "- **Sequence extraction**: Automatic protein, transcript, CDS, and gene sequence extraction\n",
        "- **Geneious compatibility**: Creates viewer-compatible GFF files\n",
        "- **Genome quality assessment**: N50/N90 analysis with assembly quality classification\n",
        "\n",
        "**Supported Genomes:**\n",
        "- Minimum sequence length: 25 kbp per record\n",
        "- Maximum recommended file size: 1 GB\n",
        "- Supports compressed FASTA files (.gz, .zip, .bz2)\n",
        "- Automatically optimizes parameters for assembly quality\n",
        "\n",
        "Felix Holst, Anthony Bolger, Christopher G√ºnther, Janina Ma√ü, Sebastian Triesch, Felicitas Kindel, Niklas Kiel, Nima Saadat, Oliver Ebenh√∂h, Bj√∂rn Usadel, Rainer Schwacke, Marie Bolger, Andreas P.M. Weber, Alisandra K. Denton. Helixer‚Äîde novo Prediction of Primary Eukaryotic Gene Models. 2023. *bioRxiv*, [10.1101/2023.02.06.527280](https://doi.org/10.1101/2023.02.06.527280)\n",
        "\n",
        "Felix Stiehler, Marvin Steinborn, Stephan Scholz, Daniela Dey, Andreas P M Weber, Alisandra K Denton. 2020. Helixer: Cross-species gene annotation of large eukaryotic genomes using deep learning. *Bioinformatics*, [btaa1044](https://doi.org/10.1093/bioinformatics/btaa1044)"
      ],
      "metadata": {
        "id": "mH8ZI4ZhtnHc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "RQ_PiinhtiEj",
        "outputId": "39a58802-8d70-4cde-9355-680ff3ea2214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Updating system packages]\n",
            "‚úÖ OK\n",
            "[Installing system dependencies]\n",
            "‚úÖ OK\n",
            "[Installing Rust for HelixerPost]\n",
            "[Installing Rust]\n",
            "‚úÖ OK\n",
            "[Cloning Helixer repository]\n",
            "‚úÖ OK\n",
            "[Cloning HelixerPost repository]\n",
            "‚úÖ OK\n",
            "[Building HelixerPost with Rust]\n",
            "‚úÖ OK\n",
            "[Installing HelixerPost binary]\n",
            "‚úÖ OK\n",
            "[Upgrading pip and installing wheel]\n",
            "‚úÖ OK\n",
            "[Installing Python requirements]\n",
            "‚úÖ OK\n",
            "[Installing Helixer]\n",
            "‚úÖ OK\n",
            "[Installing TensorFlow with GPU support]\n",
            "‚úÖ OK\n",
            "[Verifying GPU support]\n",
            "‚ö†Ô∏è  GPU verification error: maximum recursion depth exceeded while calling a Python object\n",
            "\n",
            "üéâ Helixer installation completed successfully!\n",
            "\n",
            "üìã Installation Summary:\n",
            "‚úÖ System dependencies installed\n",
            "‚úÖ gffread installed for sequence extraction\n",
            "‚úÖ Helixer repository cloned\n",
            "‚úÖ HelixerPost processor built and installed\n",
            "‚úÖ Python requirements installed\n",
            "‚úÖ Helixer package installed\n",
            "‚úÖ TensorFlow with GPU support installed\n",
            "\n",
            "üöÄ Ready to run Helixer predictions!\n"
          ]
        }
      ],
      "source": [
        "# CELL 2: Installation and Setup\n",
        "#@title Install Helixer and Dependencies\n",
        "#@markdown This cell installs all required dependencies for Helixer including system packages, Python libraries, gffread, and the HelixerPost processor.\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def run_command(cmd, description):\n",
        "    \"\"\"Run command with error handling and progress reporting.\"\"\"\n",
        "    print(f\"[{description}]\")\n",
        "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"‚ùå FAILED: {result.stderr[:500]}\")\n",
        "        return False\n",
        "    print(\"‚úÖ OK\")\n",
        "    return True\n",
        "\n",
        "def install_helixer():\n",
        "    \"\"\"Complete Helixer installation process.\"\"\"\n",
        "\n",
        "    # Update system packages\n",
        "    if not run_command(\"apt-get update -qq\", \"Updating system packages\"):\n",
        "        return False\n",
        "\n",
        "    # Install system dependencies including Rust and gffread\n",
        "    if not run_command(\"apt-get install -y python3-dev build-essential git wget curl gffread\", \"Installing system dependencies\"):\n",
        "        return False\n",
        "\n",
        "    # Install Rust (required for HelixerPost)\n",
        "    print(\"[Installing Rust for HelixerPost]\")\n",
        "    if not os.path.exists(\"/root/.cargo/bin/cargo\"):\n",
        "        if not run_command(\"curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\", \"Installing Rust\"):\n",
        "            return False\n",
        "        # Add Rust to PATH\n",
        "        os.environ['PATH'] = f\"/root/.cargo/bin:{os.environ['PATH']}\"\n",
        "    else:\n",
        "        print(\"‚úÖ Rust already installed\")\n",
        "\n",
        "    # Clone Helixer repository\n",
        "    if not os.path.exists(\"/content/Helixer\"):\n",
        "        if not run_command(\"cd /content && git clone https://github.com/weberlab-hhu/Helixer.git\", \"Cloning Helixer repository\"):\n",
        "            return False\n",
        "    else:\n",
        "        print(\"‚úÖ Helixer repository already exists\")\n",
        "\n",
        "    # Clone and install HelixerPost\n",
        "    if not os.path.exists(\"/content/HelixerPost\"):\n",
        "        if not run_command(\"cd /content && git clone https://github.com/TonyBolger/HelixerPost.git\", \"Cloning HelixerPost repository\"):\n",
        "            return False\n",
        "    else:\n",
        "        print(\"‚úÖ HelixerPost repository already exists\")\n",
        "\n",
        "    # Build HelixerPost with Cargo\n",
        "    if not run_command(\"cd /content/HelixerPost && /root/.cargo/bin/cargo build --release\", \"Building HelixerPost with Rust\"):\n",
        "        return False\n",
        "\n",
        "    # Install HelixerPost binary\n",
        "    helixer_post_src = \"/content/HelixerPost/target/release/helixer_post_bin\"\n",
        "    if os.path.exists(helixer_post_src):\n",
        "        if not run_command(f\"cp {helixer_post_src} /usr/local/bin/\", \"Installing HelixerPost binary\"):\n",
        "            return False\n",
        "    else:\n",
        "        print(\"‚ùå HelixerPost binary not found after build\")\n",
        "        return False\n",
        "\n",
        "    # Upgrade pip and install wheel\n",
        "    if not run_command(f\"{sys.executable} -m pip install --upgrade pip wheel\", \"Upgrading pip and installing wheel\"):\n",
        "        return False\n",
        "\n",
        "    # Install Python requirements\n",
        "    if not run_command(f\"cd /content/Helixer && {sys.executable} -m pip install -r requirements.3.10.txt\", \"Installing Python requirements\"):\n",
        "        return False\n",
        "\n",
        "    # Install Helixer\n",
        "    if not run_command(f\"cd /content/Helixer && {sys.executable} -m pip install .\", \"Installing Helixer\"):\n",
        "        return False\n",
        "\n",
        "    # Install TensorFlow with GPU support\n",
        "    if not run_command(f\"{sys.executable} -m pip install tensorflow[and-cuda]\", \"Installing TensorFlow with GPU support\"):\n",
        "        return False\n",
        "\n",
        "    # Verify GPU availability\n",
        "    print(\"[Verifying GPU support]\")\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "        gpus = tf.config.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            print(f\"‚úÖ GPU detected: {len(gpus)} device(s)\")\n",
        "            for gpu in gpus:\n",
        "                print(f\"   - {gpu}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  No GPU detected - predictions will be slow\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  GPU verification error: {e}\")\n",
        "\n",
        "    # Add Helixer scripts to PATH\n",
        "    helixer_scripts = \"/content/Helixer\"\n",
        "    if helixer_scripts not in os.environ.get('PATH', ''):\n",
        "        os.environ['PATH'] = f\"{helixer_scripts}:{os.environ['PATH']}\"\n",
        "\n",
        "    print(\"\\nüéâ Helixer installation completed successfully!\")\n",
        "    return True\n",
        "\n",
        "# Execute installation\n",
        "if install_helixer():\n",
        "    print(\"\\nüìã Installation Summary:\")\n",
        "    print(\"‚úÖ System dependencies installed\")\n",
        "    print(\"‚úÖ gffread installed for sequence extraction\")\n",
        "    print(\"‚úÖ Helixer repository cloned\")\n",
        "    print(\"‚úÖ HelixerPost processor built and installed\")\n",
        "    print(\"‚úÖ Python requirements installed\")\n",
        "    print(\"‚úÖ Helixer package installed\")\n",
        "    print(\"‚úÖ TensorFlow with GPU support installed\")\n",
        "    print(\"\\nüöÄ Ready to run Helixer predictions!\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Installation failed. Please check error messages above.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Google Drive Setup and File Input Configuration\n",
        "#@title Setup Google Drive and Configure Input Files\n",
        "#@markdown Authenticate with Google Drive, specify input genome location, and configure output folder.\n",
        "\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload\n",
        "from google.colab import files\n",
        "import gzip\n",
        "import zipfile\n",
        "import bz2\n",
        "import io\n",
        "import re\n",
        "\n",
        "# Input method selection\n",
        "input_method = \"Google Drive Path\" #@param [\"Upload File\", \"Google Drive Path\"]\n",
        "#@markdown Choose whether to upload a file or use an existing file in Google Drive\n",
        "\n",
        "# Google Drive file path (only used if \"Google Drive Path\" selected)\n",
        "gdrive_file_path = \"/Fusarium/GCA_001702875.1_Fol004_genomic.fasta.gz\" #@param {type:\"string\"}\n",
        "#@markdown Google Drive path to genome file. Format: /folder/subfolder/filename.fasta\n",
        "\n",
        "# Output folder configuration\n",
        "output_folder_name = \"Helixer_Output\" #@param {type:\"string\"}\n",
        "#@markdown Name of the output folder in Google Drive root\n",
        "\n",
        "# Global variables\n",
        "drive_service = None\n",
        "helixer_folder_id = None\n",
        "input_fasta_path = None\n",
        "input_species_name = None\n",
        "genome_stats = None\n",
        "\n",
        "def calculate_genome_statistics(fasta_path):\n",
        "    \"\"\"Calculate comprehensive genome assembly statistics including N50/N90.\"\"\"\n",
        "    print(\"üìä Calculating genome assembly statistics...\")\n",
        "\n",
        "    # Read all sequences and their lengths\n",
        "    sequence_lengths = []\n",
        "    total_length = 0\n",
        "    total_ungapped_length = 0\n",
        "    total_n_count = 0\n",
        "    total_gc_count = 0\n",
        "    total_at_count = 0\n",
        "    sequence_count = 0\n",
        "\n",
        "    with open(fasta_path, 'r') as f:\n",
        "        current_seq_length = 0\n",
        "        current_seq = \"\"\n",
        "\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line.startswith('>'):\n",
        "                # Process previous sequence\n",
        "                if current_seq:\n",
        "                    sequence_lengths.append(current_seq_length)\n",
        "                    total_length += current_seq_length\n",
        "\n",
        "                    # Count Ns, GC, AT\n",
        "                    n_count = current_seq.upper().count('N')\n",
        "                    gc_count = current_seq.upper().count('G') + current_seq.upper().count('C')\n",
        "                    at_count = current_seq.upper().count('A') + current_seq.upper().count('T')\n",
        "\n",
        "                    total_n_count += n_count\n",
        "                    total_gc_count += gc_count\n",
        "                    total_at_count += at_count\n",
        "                    total_ungapped_length += (current_seq_length - n_count)\n",
        "                    sequence_count += 1\n",
        "\n",
        "                # Reset for new sequence\n",
        "                current_seq_length = 0\n",
        "                current_seq = \"\"\n",
        "            else:\n",
        "                current_seq_length += len(line)\n",
        "                current_seq += line\n",
        "\n",
        "        # Process last sequence\n",
        "        if current_seq:\n",
        "            sequence_lengths.append(current_seq_length)\n",
        "            total_length += current_seq_length\n",
        "\n",
        "            n_count = current_seq.upper().count('N')\n",
        "            gc_count = current_seq.upper().count('G') + current_seq.upper().count('C')\n",
        "            at_count = current_seq.upper().count('A') + current_seq.upper().count('T')\n",
        "\n",
        "            total_n_count += n_count\n",
        "            total_gc_count += gc_count\n",
        "            total_at_count += at_count\n",
        "            total_ungapped_length += (current_seq_length - n_count)\n",
        "            sequence_count += 1\n",
        "\n",
        "    # Sort sequences by length (largest first) for N50/N90 calculation\n",
        "    sequence_lengths.sort(reverse=True)\n",
        "\n",
        "    # Calculate N50 and N90\n",
        "    def calculate_nx(lengths, percentage):\n",
        "        target_length = sum(lengths) * (percentage / 100.0)\n",
        "        cumulative_length = 0\n",
        "        lx_count = 0\n",
        "\n",
        "        for length in lengths:\n",
        "            cumulative_length += length\n",
        "            lx_count += 1\n",
        "            if cumulative_length >= target_length:\n",
        "                return length, lx_count\n",
        "        return lengths[-1] if lengths else 0, len(lengths)\n",
        "\n",
        "    n50, l50 = calculate_nx(sequence_lengths, 50)\n",
        "    n90, l90 = calculate_nx(sequence_lengths, 90)\n",
        "\n",
        "    # Calculate percentages\n",
        "    gc_percent = (total_gc_count / (total_gc_count + total_at_count)) * 100 if (total_gc_count + total_at_count) > 0 else 0\n",
        "    gap_percent = (total_n_count / total_length) * 100 if total_length > 0 else 0\n",
        "\n",
        "    # Classify assembly quality\n",
        "    if n50 >= 1_000_000:  # >= 1 Mb\n",
        "        quality_class = \"Excellent\"\n",
        "        quality_desc = \"Chromosome-level\"\n",
        "        quality_emoji = \"üèÜ\"\n",
        "    elif n50 >= 100_000:  # >= 100 kb\n",
        "        quality_class = \"Good\"\n",
        "        quality_desc = \"Scaffold-level\"\n",
        "        quality_emoji = \"ü•à\"\n",
        "    elif n50 >= 10_000:   # >= 10 kb\n",
        "        quality_class = \"Fair\"\n",
        "        quality_desc = \"Contig-level\"\n",
        "        quality_emoji = \"ü•â\"\n",
        "    else:\n",
        "        quality_class = \"Poor\"\n",
        "        quality_desc = \"Fragmented\"\n",
        "        quality_emoji = \"‚ö†Ô∏è\"\n",
        "\n",
        "    # Create statistics dictionary\n",
        "    stats = {\n",
        "        'total_length': total_length,\n",
        "        'ungapped_length': total_ungapped_length,\n",
        "        'sequence_count': sequence_count,\n",
        "        'n50': n50,\n",
        "        'n90': n90,\n",
        "        'l50': l50,\n",
        "        'l90': l90,\n",
        "        'longest_sequence': max(sequence_lengths) if sequence_lengths else 0,\n",
        "        'shortest_sequence': min(sequence_lengths) if sequence_lengths else 0,\n",
        "        'average_length': total_length / sequence_count if sequence_count > 0 else 0,\n",
        "        'median_length': sequence_lengths[len(sequence_lengths)//2] if sequence_lengths else 0,\n",
        "        'gc_percent': gc_percent,\n",
        "        'gap_percent': gap_percent,\n",
        "        'quality_class': quality_class,\n",
        "        'quality_desc': quality_desc,\n",
        "        'quality_emoji': quality_emoji\n",
        "    }\n",
        "\n",
        "    return stats\n",
        "\n",
        "def display_genome_statistics(stats):\n",
        "    \"\"\"Display formatted genome statistics.\"\"\"\n",
        "    print(f\"\\nüìä Genome Assembly Statistics:\")\n",
        "    print(f\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\")\n",
        "    print(f\"{stats['quality_emoji']} Assembly Quality: {stats['quality_class']} ({stats['quality_desc']})\")\n",
        "    print(f\"üìè Genome size: {stats['total_length'] / 1_000_000:.1f} Mb\")\n",
        "    print(f\"üìê Ungapped length: {stats['ungapped_length'] / 1_000_000:.1f} Mb ({100 - stats['gap_percent']:.1f}%)\")\n",
        "    print(f\"üß© Number of sequences: {stats['sequence_count']:,}\")\n",
        "    print(f\"üìä Sequence N50: {stats['n50'] / 1000:.1f} kb\")\n",
        "    print(f\"üìä Sequence N90: {stats['n90'] / 1000:.1f} kb\")\n",
        "    print(f\"üî¢ Sequence L50: {stats['l50']:,}\")\n",
        "    print(f\"üî¢ Sequence L90: {stats['l90']:,}\")\n",
        "    print(f\"üìè Longest sequence: {stats['longest_sequence'] / 1000:.1f} kb\")\n",
        "    print(f\"üìè Average sequence: {stats['average_length'] / 1000:.1f} kb\")\n",
        "    print(f\"üß™ GC content: {stats['gc_percent']:.1f}%\")\n",
        "    if stats['gap_percent'] > 0:\n",
        "        print(f\"üï≥Ô∏è  Gap content: {stats['gap_percent']:.1f}%\")\n",
        "\n",
        "def setup_google_drive():\n",
        "    \"\"\"Setup Google Drive authentication and output folder.\"\"\"\n",
        "    global drive_service, helixer_folder_id\n",
        "\n",
        "    print(\"üîê Authenticating with Google Drive...\")\n",
        "    try:\n",
        "        auth.authenticate_user()\n",
        "        drive_service = build('drive', 'v3')\n",
        "        print(\"‚úÖ Google Drive authentication successful\")\n",
        "\n",
        "        # Create or find output folder\n",
        "        print(f\"üìÅ Setting up output folder: {output_folder_name}\")\n",
        "\n",
        "        # Search for existing folder\n",
        "        query = f\"name='{output_folder_name}' and mimeType='application/vnd.google-apps.folder' and trashed=false\"\n",
        "        results = drive_service.files().list(q=query).execute()\n",
        "        folders = results.get('files', [])\n",
        "\n",
        "        if folders:\n",
        "            helixer_folder_id = folders[0]['id']\n",
        "            print(f\"‚úÖ Found existing folder: {output_folder_name}\")\n",
        "        else:\n",
        "            # Create new folder\n",
        "            folder_metadata = {\n",
        "                'name': output_folder_name,\n",
        "                'mimeType': 'application/vnd.google-apps.folder'\n",
        "            }\n",
        "            folder = drive_service.files().create(body=folder_metadata).execute()\n",
        "            helixer_folder_id = folder['id']\n",
        "            print(f\"‚úÖ Created new folder: {output_folder_name}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Google Drive setup failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def find_file_by_path(path):\n",
        "    \"\"\"Find file in Google Drive by path.\"\"\"\n",
        "    if not drive_service:\n",
        "        return None\n",
        "\n",
        "    # Clean and split path\n",
        "    path = path.strip().strip('/')\n",
        "    if not path:\n",
        "        return None\n",
        "\n",
        "    path_parts = path.split('/')\n",
        "    filename = path_parts[-1]\n",
        "    folder_path = path_parts[:-1] if len(path_parts) > 1 else []\n",
        "\n",
        "    print(f\"üîç Searching for: {filename}\")\n",
        "    if folder_path:\n",
        "        print(f\"   üìÅ In folder path: /{'/'.join(folder_path)}\")\n",
        "\n",
        "    try:\n",
        "        # Navigate through folder structure\n",
        "        current_parent = 'root'\n",
        "\n",
        "        for folder_name in folder_path:\n",
        "            query = f\"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and '{current_parent}' in parents and trashed=false\"\n",
        "            results = drive_service.files().list(q=query).execute()\n",
        "            folders = results.get('files', [])\n",
        "\n",
        "            if not folders:\n",
        "                print(f\"‚ùå Folder not found: {folder_name}\")\n",
        "                return None\n",
        "\n",
        "            current_parent = folders[0]['id']\n",
        "            print(f\"   ‚úÖ Found folder: {folder_name}\")\n",
        "\n",
        "        # Search for file in final folder\n",
        "        query = f\"name='{filename}' and '{current_parent}' in parents and trashed=false\"\n",
        "        results = drive_service.files().list(q=query).execute()\n",
        "        files_found = results.get('files', [])\n",
        "\n",
        "        if not files_found:\n",
        "            print(f\"‚ùå File not found: {filename}\")\n",
        "            return None\n",
        "\n",
        "        if len(files_found) > 1:\n",
        "            print(f\"‚ö†Ô∏è  Multiple files found with name: {filename}\")\n",
        "            print(\"   Using the first one found\")\n",
        "\n",
        "        file_info = files_found[0]\n",
        "        print(f\"‚úÖ Found file: {file_info['name']} (ID: {file_info['id']})\")\n",
        "        return file_info\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error searching for file: {e}\")\n",
        "        return None\n",
        "\n",
        "def download_from_gdrive(file_info):\n",
        "    \"\"\"Download file from Google Drive.\"\"\"\n",
        "    try:\n",
        "        print(f\"‚¨áÔ∏è  Downloading {file_info['name']} from Google Drive...\")\n",
        "\n",
        "        request = drive_service.files().get_media(fileId=file_info['id'])\n",
        "        downloaded_content = io.BytesIO()\n",
        "        downloader = MediaIoBaseDownload(downloaded_content, request)\n",
        "\n",
        "        done = False\n",
        "        while done is False:\n",
        "            status, done = downloader.next_chunk()\n",
        "            if status:\n",
        "                progress = int(status.progress() * 100)\n",
        "                if progress % 20 == 0:  # Show progress every 20%\n",
        "                    print(f\"   üìä Progress: {progress}%\")\n",
        "\n",
        "        # Save to local file\n",
        "        local_filename = file_info['name']\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            f.write(downloaded_content.getvalue())\n",
        "\n",
        "        file_size_mb = os.path.getsize(local_filename) / (1024 * 1024)\n",
        "        print(f\"‚úÖ Downloaded: {local_filename} ({file_size_mb:.1f} MB)\")\n",
        "        return local_filename\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Download failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def handle_file_upload():\n",
        "    \"\"\"Handle local file upload with compression support.\"\"\"\n",
        "    print(\"üìÅ Upload your genome FASTA file:\")\n",
        "    print(\"   üìã Supported formats:\")\n",
        "    print(\"      ‚Ä¢ FASTA: .fasta, .fa, .fas, .fna\")\n",
        "    print(\"      ‚Ä¢ Compressed: .gz, .zip, .bz2\")\n",
        "    print(\"   üìè Requirements:\")\n",
        "    print(\"      ‚Ä¢ Minimum sequence length: 25 kbp per record\")\n",
        "    print(\"      ‚Ä¢ Maximum file size: 1 GB\")\n",
        "    print(\"      ‚Ä¢ Valid FASTA format with '>' headers\")\n",
        "    print()\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"‚ùå No file uploaded\")\n",
        "        return None\n",
        "\n",
        "    uploaded_filename = list(uploaded.keys())[0]\n",
        "    print(f\"üìã Processing uploaded file: {uploaded_filename}\")\n",
        "\n",
        "    return process_fasta_file(uploaded_filename)\n",
        "\n",
        "def process_fasta_file(filename):\n",
        "    \"\"\"Process FASTA file with decompression if needed.\"\"\"\n",
        "    global input_fasta_path, input_species_name, genome_stats\n",
        "\n",
        "    # Determine file type and extract if needed\n",
        "    temp_fasta_path = None\n",
        "\n",
        "    try:\n",
        "        if filename.endswith('.gz'):\n",
        "            print(\"üóúÔ∏è  Decompressing .gz file...\")\n",
        "            with gzip.open(filename, 'rt') as f:\n",
        "                content = f.read()\n",
        "            temp_fasta_path = filename.replace('.gz', '')\n",
        "            with open(temp_fasta_path, 'w') as f:\n",
        "                f.write(content)\n",
        "\n",
        "        elif filename.endswith('.zip'):\n",
        "            print(\"üóúÔ∏è  Extracting .zip file...\")\n",
        "            with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "                zip_ref.extractall('.')\n",
        "                # Find FASTA file in extracted files\n",
        "                for file in zip_ref.namelist():\n",
        "                    if file.lower().endswith(('.fasta', '.fa', '.fas', '.fna')):\n",
        "                        temp_fasta_path = file\n",
        "                        break\n",
        "\n",
        "        elif filename.endswith('.bz2'):\n",
        "            print(\"üóúÔ∏è  Decompressing .bz2 file...\")\n",
        "            with bz2.open(filename, 'rt') as f:\n",
        "                content = f.read()\n",
        "            temp_fasta_path = filename.replace('.bz2', '')\n",
        "            with open(temp_fasta_path, 'w') as f:\n",
        "                f.write(content)\n",
        "\n",
        "        else:\n",
        "            # Assume uncompressed FASTA\n",
        "            temp_fasta_path = filename\n",
        "\n",
        "        if not temp_fasta_path or not os.path.exists(temp_fasta_path):\n",
        "            print(\"‚ùå Could not find or extract FASTA file\")\n",
        "            return None\n",
        "\n",
        "        # Validate FASTA file\n",
        "        with open(temp_fasta_path, 'r') as f:\n",
        "            first_line = f.readline().strip()\n",
        "            if not first_line.startswith('>'):\n",
        "                print(\"‚ùå File does not appear to be in FASTA format\")\n",
        "                print(\"   First line should start with '>' character\")\n",
        "                return None\n",
        "\n",
        "        # Calculate comprehensive genome statistics\n",
        "        genome_stats = calculate_genome_statistics(temp_fasta_path)\n",
        "\n",
        "        # Display statistics\n",
        "        display_genome_statistics(genome_stats)\n",
        "\n",
        "        # Check minimum sequence length requirement\n",
        "        if genome_stats['shortest_sequence'] < 25000:\n",
        "            print(f\"\\n‚ö†Ô∏è  Warning: Shortest sequence ({genome_stats['shortest_sequence']:,} bp) is below recommended minimum (25,000 bp)\")\n",
        "            print(\"   This may affect annotation quality for short sequences\")\n",
        "\n",
        "        # Extract species name from filename - create shorter, cleaner names\n",
        "        base_name = os.path.splitext(os.path.basename(temp_fasta_path))[0]\n",
        "\n",
        "        # Create a much shorter species name\n",
        "        if 'GCA_' in base_name or 'GCF_' in base_name:\n",
        "            # For NCBI accessions, just use the first part\n",
        "            species_name = base_name.split('_')[0] + '_' + base_name.split('_')[1]\n",
        "        else:\n",
        "            # For other names, take first 2 meaningful parts\n",
        "            parts = re.sub(r'[_\\-\\.]+', ' ', base_name).split()[:2]\n",
        "            species_name = '_'.join(parts)\n",
        "\n",
        "        # Clean up further\n",
        "        species_name = re.sub(r'[^A-Za-z0-9_]', '', species_name)\n",
        "\n",
        "        # Ensure it's not empty\n",
        "        if not species_name:\n",
        "            species_name = \"unknown_species\"\n",
        "\n",
        "        input_fasta_path = temp_fasta_path\n",
        "        input_species_name = species_name\n",
        "\n",
        "        print(f\"\\nüè∑Ô∏è  Detected species name: {input_species_name}\")\n",
        "        return temp_fasta_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing FASTA file: {e}\")\n",
        "        return None\n",
        "\n",
        "def upload_to_drive(file_path, job_name, file_type=\"result\"):\n",
        "    \"\"\"Upload file to Google Drive in the output folder.\"\"\"\n",
        "    global drive_service, helixer_folder_id\n",
        "\n",
        "    if not drive_service or not helixer_folder_id:\n",
        "        print(\"‚ùå Google Drive not properly configured\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        filename = os.path.basename(file_path)\n",
        "        print(f\"‚òÅÔ∏è  Uploading {filename} to Google Drive...\")\n",
        "\n",
        "        # Create job subfolder\n",
        "        job_folder_name = f\"{job_name}_{file_type}\"\n",
        "        job_folder_metadata = {\n",
        "            'name': job_folder_name,\n",
        "            'mimeType': 'application/vnd.google-apps.folder',\n",
        "            'parents': [helixer_folder_id]\n",
        "        }\n",
        "        job_folder = drive_service.files().create(body=job_folder_metadata).execute()\n",
        "        job_folder_id = job_folder['id']\n",
        "\n",
        "        # Upload file\n",
        "        file_metadata = {\n",
        "            'name': filename,\n",
        "            'parents': [job_folder_id]\n",
        "        }\n",
        "        media = MediaFileUpload(file_path, resumable=True)\n",
        "        uploaded_file = drive_service.files().create(\n",
        "            body=file_metadata,\n",
        "            media_body=media\n",
        "        ).execute()\n",
        "\n",
        "        file_url = f\"https://drive.google.com/file/d/{uploaded_file['id']}/view\"\n",
        "        print(f\"‚úÖ Upload complete: {file_url}\")\n",
        "        return file_url\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Upload failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# Display instructions\n",
        "print(\"üìã Google Drive File Path Instructions:\")\n",
        "print(\"‚ïê\" * 50)\n",
        "print(\"If using 'Google Drive Path' method:\")\n",
        "print()\n",
        "print(\"üìÅ Path Format Examples:\")\n",
        "print(\"   ‚Ä¢ Root folder file: filename.fasta\")\n",
        "print(\"   ‚Ä¢ Subfolder file: MyFolder/genome.fasta.gz\")\n",
        "print(\"   ‚Ä¢ Nested folders: Data/Genomes/Species/assembly.fa\")\n",
        "print()\n",
        "print(\"‚úÖ Supported File Formats:\")\n",
        "print(\"   ‚Ä¢ FASTA: .fasta, .fa, .fas, .fna\")\n",
        "print(\"   ‚Ä¢ Compressed: .gz, .zip, .bz2\")\n",
        "print()\n",
        "print(\"‚ö†Ô∏è  Important Notes:\")\n",
        "print(\"   ‚Ä¢ Use forward slashes (/) for folder separation\")\n",
        "print(\"   ‚Ä¢ File names are case-sensitive\")\n",
        "print(\"   ‚Ä¢ Ensure file is not in Trash\")\n",
        "print(\"   ‚Ä¢ File must be accessible by your Google account\")\n",
        "print()\n",
        "\n",
        "# Execute setup\n",
        "print(\"üöÄ Starting Google Drive Setup...\")\n",
        "print(\"‚ïê\" * 40)\n",
        "\n",
        "if not setup_google_drive():\n",
        "    print(\"\\n‚ùå Google Drive setup failed\")\n",
        "    print(\"‚ö†Ô∏è  Cannot proceed without Google Drive access\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Google Drive setup completed!\")\n",
        "    print(f\"üìÅ Output folder: {output_folder_name}\")\n",
        "\n",
        "    # Handle input file based on method\n",
        "    if input_method == \"Upload File\":\n",
        "        print(f\"\\nüì§ File Upload Method Selected\")\n",
        "        result = handle_file_upload()\n",
        "\n",
        "    elif input_method == \"Google Drive Path\":\n",
        "        print(f\"\\n‚òÅÔ∏è  Google Drive Path Method Selected\")\n",
        "\n",
        "        if not gdrive_file_path.strip():\n",
        "            print(\"‚ùå No Google Drive path specified\")\n",
        "            print(\"   Please enter the path to your genome file\")\n",
        "            result = None\n",
        "        else:\n",
        "            print(f\"üîç Looking for file: {gdrive_file_path}\")\n",
        "            file_info = find_file_by_path(gdrive_file_path)\n",
        "\n",
        "            if file_info:\n",
        "                # Download file from Google Drive\n",
        "                local_file = download_from_gdrive(file_info)\n",
        "                if local_file:\n",
        "                    result = process_fasta_file(local_file)\n",
        "                else:\n",
        "                    result = None\n",
        "            else:\n",
        "                result = None\n",
        "\n",
        "    # Final status\n",
        "    if result:\n",
        "        print(f\"\\nüéâ Input file ready for processing!\")\n",
        "        print(f\"üìÅ Local path: {input_fasta_path}\")\n",
        "        print(f\"üè∑Ô∏è  Species: {input_species_name}\")\n",
        "        print(f\"‚òÅÔ∏è  Output folder: {output_folder_name}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ùå File setup failed\")\n",
        "        print(\"‚ö†Ô∏è  Please check your input method and try again\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "qrX_JoQAt0S2",
        "outputId": "d4c9dc7f-4c60-4d83-f3e9-43d2f9405749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìã Google Drive File Path Instructions:\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "If using 'Google Drive Path' method:\n",
            "\n",
            "üìÅ Path Format Examples:\n",
            "   ‚Ä¢ Root folder file: filename.fasta\n",
            "   ‚Ä¢ Subfolder file: MyFolder/genome.fasta.gz\n",
            "   ‚Ä¢ Nested folders: Data/Genomes/Species/assembly.fa\n",
            "\n",
            "‚úÖ Supported File Formats:\n",
            "   ‚Ä¢ FASTA: .fasta, .fa, .fas, .fna\n",
            "   ‚Ä¢ Compressed: .gz, .zip, .bz2\n",
            "\n",
            "‚ö†Ô∏è  Important Notes:\n",
            "   ‚Ä¢ Use forward slashes (/) for folder separation\n",
            "   ‚Ä¢ File names are case-sensitive\n",
            "   ‚Ä¢ Ensure file is not in Trash\n",
            "   ‚Ä¢ File must be accessible by your Google account\n",
            "\n",
            "üöÄ Starting Google Drive Setup...\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "üîê Authenticating with Google Drive...\n",
            "‚úÖ Google Drive authentication successful\n",
            "üìÅ Setting up output folder: Helixer_Output\n",
            "‚úÖ Found existing folder: Helixer_Output\n",
            "\n",
            "‚úÖ Google Drive setup completed!\n",
            "üìÅ Output folder: Helixer_Output\n",
            "\n",
            "‚òÅÔ∏è  Google Drive Path Method Selected\n",
            "üîç Looking for file: /Fusarium/GCA_001702875.1_Fol004_genomic.fasta.gz\n",
            "üîç Searching for: GCA_001702875.1_Fol004_genomic.fasta.gz\n",
            "   üìÅ In folder path: /Fusarium\n",
            "   ‚úÖ Found folder: Fusarium\n",
            "‚úÖ Found file: GCA_001702875.1_Fol004_genomic.fasta.gz (ID: 1fG1AIC-Wfuylxcn2ij3deQDEmnXLw6Hb)\n",
            "‚¨áÔ∏è  Downloading GCA_001702875.1_Fol004_genomic.fasta.gz from Google Drive...\n",
            "   üìä Progress: 100%\n",
            "‚úÖ Downloaded: GCA_001702875.1_Fol004_genomic.fasta.gz (14.4 MB)\n",
            "üóúÔ∏è  Decompressing .gz file...\n",
            "üìä Calculating genome assembly statistics...\n",
            "\n",
            "üìä Genome Assembly Statistics:\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "ü•à Assembly Quality: Good (Scaffold-level)\n",
            "üìè Genome size: 51.9 Mb\n",
            "üìê Ungapped length: 51.9 Mb (100.0%)\n",
            "üß© Number of sequences: 3,433\n",
            "üìä Sequence N50: 152.3 kb\n",
            "üìä Sequence N90: 6.8 kb\n",
            "üî¢ Sequence L50: 88\n",
            "üî¢ Sequence L90: 719\n",
            "üìè Longest sequence: 878.9 kb\n",
            "üìè Average sequence: 15.1 kb\n",
            "üß™ GC content: 47.8%\n",
            "üï≥Ô∏è  Gap content: 0.0%\n",
            "\n",
            "‚ö†Ô∏è  Warning: Shortest sequence (500 bp) is below recommended minimum (25,000 bp)\n",
            "   This may affect annotation quality for short sequences\n",
            "\n",
            "üè∑Ô∏è  Detected species name: GCA_0017028751\n",
            "\n",
            "üéâ Input file ready for processing!\n",
            "üìÅ Local path: GCA_001702875.1_Fol004_genomic.fasta\n",
            "üè∑Ô∏è  Species: GCA_0017028751\n",
            "‚òÅÔ∏è  Output folder: Helixer_Output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: Job Configuration and Parameters\n",
        "#@title Configure Helixer Parameters\n",
        "#@markdown Set job name, lineage model, and prediction parameters. Parameters are intelligently recommended based on your genome assembly quality and lineage.\n",
        "\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# Job configuration\n",
        "job_name = \"helixer_annotation\" #@param {type:\"string\"}\n",
        "#@markdown Job name for organizing output files\n",
        "\n",
        "species_name_override = \"\" #@param {type:\"string\"}\n",
        "#@markdown Override detected species name (leave empty to use detected name)\n",
        "\n",
        "gff_feature_label = \"Helixer\" #@param {type:\"string\"}\n",
        "#@markdown Label for GFF feature naming (appears in the source column)\n",
        "\n",
        "# Model selection\n",
        "lineage = \"fungi\" #@param [\"land_plant\", \"vertebrate\", \"invertebrate\", \"fungi\"]\n",
        "#@markdown Choose the lineage model appropriate for your species\n",
        "\n",
        "# Parameter automation mode\n",
        "parameter_mode = \"Auto-Optimized\" #@param [\"Auto-Optimized\", \"Conservative\", \"Manual\"]\n",
        "#@markdown **Auto-Optimized**: Best settings for your assembly quality and lineage (recommended). **Conservative**: Safe settings for any assembly. **Manual**: Full control over all parameters.\n",
        "\n",
        "# Core prediction parameters (Manual mode only)\n",
        "peak_threshold = 0.6 #@param {type:\"slider\", min:0.1, max:1.0, step:0.05}\n",
        "#@markdown **Peak threshold**: Minimum genic score to accept a gene candidate. Lower values (0.6) increase sensitivity for effector annotation. Higher values (0.8-0.95) increase precision.\n",
        "\n",
        "edge_threshold = 0.1 #@param {type:\"slider\", min:0.05, max:0.5, step:0.05}\n",
        "#@markdown **Edge threshold**: Genic score threshold for defining start/end boundaries of candidate regions within the sliding window.\n",
        "\n",
        "min_coding_length = 60 #@param {type:\"integer\"}\n",
        "#@markdown **Minimum coding length**: Filter out genes with total coding sequence shorter than this value (bp). Recommended: 60 bp for effectors, 150+ bp for standard annotation.\n",
        "\n",
        "window_size = 100 #@param {type:\"integer\"}\n",
        "#@markdown **Window size**: Width of sliding window assessed for intergenic vs genic content. Default 100 bp works well for most genomes.\n",
        "\n",
        "# Advanced parameters (Manual mode only)\n",
        "subsequence_length = \"auto\" #@param [\"auto\", \"21384\", \"64152\", \"106920\", \"213840\", \"320760\", \"custom\"]\n",
        "#@markdown **Subsequence length**: How much genome the neural network sees at once. Should be longer than typical genes. Auto-selected based on lineage and assembly quality.\n",
        "\n",
        "custom_subsequence_length = 64152 #@param {type:\"integer\"}\n",
        "#@markdown **Custom subsequence length**: Used only if 'custom' selected above. Must be divisible by 9.\n",
        "\n",
        "batch_size = \"auto\" #@param [\"auto\", \"8\", \"16\", \"32\", \"64\", \"128\"]\n",
        "#@markdown **Batch size**: Number of subsequences processed simultaneously. Larger values are faster but require more GPU memory. Auto-selected for optimal performance.\n",
        "\n",
        "use_overlap = True #@param {type:\"boolean\"}\n",
        "#@markdown **Use overlap**: Create overlapping sliding-window predictions for better quality at subsequence boundaries. Increases processing time but improves accuracy.\n",
        "\n",
        "predict_phase = True #@param {type:\"boolean\"}\n",
        "#@markdown **Predict phase**: Also predict CDS reading frames/phases. Recommended for complete gene models and downstream analysis.\n",
        "\n",
        "verbose_output = True #@param {type:\"boolean\"}\n",
        "#@markdown **Verbose output**: Enable detailed progress information during processing.\n",
        "\n",
        "def get_intelligent_parameters(lineage, assembly_stats, mode=\"Auto-Optimized\"):\n",
        "    \"\"\"Get intelligent parameter recommendations based on lineage and assembly quality.\"\"\"\n",
        "\n",
        "    if not assembly_stats:\n",
        "        print(\"‚ö†Ô∏è  No assembly statistics available, using conservative defaults\")\n",
        "        return get_conservative_parameters(lineage)\n",
        "\n",
        "    n50 = assembly_stats['n50']\n",
        "    quality_class = assembly_stats['quality_class']\n",
        "\n",
        "    # Define lineage-specific parameter matrices\n",
        "    # Format: [subsequence_length, batch_size, overlap_offset, overlap_core_length]\n",
        "\n",
        "    lineage_configs = {\n",
        "        \"fungi\": {\n",
        "            \"conservative\":   [21384,  64,  10692,  16038],   # GitHub defaults\n",
        "            \"standard\":       [64152,  128, 32076,  48114],   # 3x faster, good assemblies\n",
        "            \"aggressive\":     [106920, 128, 53460,  80190],   # 5x faster, excellent assemblies\n",
        "        },\n",
        "        \"land_plant\": {\n",
        "            \"conservative\":   [64152,  64,  32076,  48114],   # GitHub defaults\n",
        "            \"standard\":       [106920, 64,  53460,  80190],   # GitHub \"try up to\"\n",
        "            \"aggressive\":     [213840, 32,  106920, 160380],  # Vertebrate-level for excellent assemblies\n",
        "        },\n",
        "        \"vertebrate\": {\n",
        "            \"conservative\":   [106920, 32,  53460,  80190],   # For fragmented assemblies\n",
        "            \"standard\":       [213840, 32,  106920, 160380],  # GitHub defaults\n",
        "            \"aggressive\":     [320760, 16,  160380, 240570],  # For chromosome-level assemblies\n",
        "        },\n",
        "        \"invertebrate\": {\n",
        "            \"conservative\":   [106920, 32,  53460,  80190],   # For fragmented assemblies\n",
        "            \"standard\":       [213840, 32,  106920, 160380],  # GitHub defaults\n",
        "            \"aggressive\":     [320760, 16,  160380, 240570],  # For chromosome-level assemblies\n",
        "        }\n",
        "    }\n",
        "\n",
        "    config = lineage_configs.get(lineage, lineage_configs[\"land_plant\"])\n",
        "\n",
        "    # Select parameter set based on assembly quality and mode\n",
        "    if mode == \"Conservative\":\n",
        "        params = config[\"conservative\"]\n",
        "        param_desc = \"Conservative (safe for any assembly)\"\n",
        "    elif mode == \"Auto-Optimized\":\n",
        "        # Intelligent selection based on N50\n",
        "        if quality_class == \"Excellent\":  # N50 > 1 Mb\n",
        "            params = config[\"aggressive\"]\n",
        "            param_desc = f\"Aggressive (optimized for {quality_class.lower()} assembly)\"\n",
        "        elif quality_class == \"Good\":     # N50 100kb-1Mb\n",
        "            params = config[\"standard\"]\n",
        "            param_desc = f\"Standard (optimized for {quality_class.lower()} assembly)\"\n",
        "        else:                            # N50 < 100kb\n",
        "            params = config[\"conservative\"]\n",
        "            param_desc = f\"Conservative (safe for {quality_class.lower()} assembly)\"\n",
        "    else:  # Manual\n",
        "        return None, None\n",
        "\n",
        "    # Validate that subsequence length doesn't exceed N50\n",
        "    subsequence_length, batch_size, overlap_offset, overlap_core_length = params\n",
        "\n",
        "    # Safety check: reduce if subsequence length > 50% of N50\n",
        "    max_safe_length = int(n50 * 0.5)\n",
        "    if subsequence_length > max_safe_length:\n",
        "        print(f\"‚ö†Ô∏è  Reducing subsequence length from {subsequence_length:,} to {max_safe_length:,} bp (50% of N50)\")\n",
        "        subsequence_length = max_safe_length\n",
        "        # Recalculate dependent parameters\n",
        "        overlap_offset = subsequence_length // 2\n",
        "        overlap_core_length = int(subsequence_length * 0.75)\n",
        "        param_desc += \" (N50-adjusted)\"\n",
        "\n",
        "    return {\n",
        "        'subsequence_length': subsequence_length,\n",
        "        'batch_size': batch_size,\n",
        "        'overlap_offset': overlap_offset,\n",
        "        'overlap_core_length': overlap_core_length,\n",
        "        'description': param_desc\n",
        "    }, param_desc\n",
        "\n",
        "def get_conservative_parameters(lineage):\n",
        "    \"\"\"Get conservative parameters that work for any assembly quality.\"\"\"\n",
        "    conservative_params = {\n",
        "        \"fungi\": [21384, 64, 10692, 16038],\n",
        "        \"land_plant\": [64152, 64, 32076, 48114],\n",
        "        \"vertebrate\": [106920, 32, 53460, 80190],\n",
        "        \"invertebrate\": [106920, 32, 53460, 80190]\n",
        "    }\n",
        "\n",
        "    params = conservative_params.get(lineage, conservative_params[\"land_plant\"])\n",
        "    subsequence_length, batch_size, overlap_offset, overlap_core_length = params\n",
        "\n",
        "    return {\n",
        "        'subsequence_length': subsequence_length,\n",
        "        'batch_size': batch_size,\n",
        "        'overlap_offset': overlap_offset,\n",
        "        'overlap_core_length': overlap_core_length,\n",
        "        'description': \"Conservative (safe for any assembly)\"\n",
        "    }, \"Conservative (safe for any assembly)\"\n",
        "\n",
        "def validate_and_prepare_config():\n",
        "    \"\"\"Validate parameters and prepare configuration.\"\"\"\n",
        "    global job_name, species_name_override, input_species_name, genome_stats\n",
        "\n",
        "    # Validate required inputs\n",
        "    if not input_fasta_path:\n",
        "        print(\"‚ùå No input file uploaded. Please run the file upload cell first.\")\n",
        "        return False\n",
        "\n",
        "    # Clean job name\n",
        "    job_name_clean = re.sub(r'[^\\w\\-_]', '_', job_name.strip())\n",
        "    if not job_name_clean:\n",
        "        job_name_clean = f\"helixer_job_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "    # Set species name\n",
        "    final_species = species_name_override.strip() if species_name_override.strip() else input_species_name\n",
        "\n",
        "    # Get intelligent parameter recommendations\n",
        "    if parameter_mode in [\"Auto-Optimized\", \"Conservative\"]:\n",
        "        optimal_params, param_desc = get_intelligent_parameters(lineage, genome_stats, parameter_mode)\n",
        "\n",
        "        if optimal_params:\n",
        "            print(f\"\\nüéØ Parameter Optimization:\")\n",
        "            print(f\"   üìä Assembly quality: {genome_stats['quality_class']} (N50: {genome_stats['n50']/1000:.1f} kb)\")\n",
        "            print(f\"   üåø Lineage: {lineage}\")\n",
        "            print(f\"   ‚öôÔ∏è  Selected: {param_desc}\")\n",
        "            print(f\"   üìè Subsequence length: {optimal_params['subsequence_length']:,} bp\")\n",
        "            print(f\"   üöÄ Expected speed improvement: {optimal_params['subsequence_length'] // 21384:.1f}x faster than minimum settings\")\n",
        "\n",
        "            final_subsequence_length = optimal_params['subsequence_length']\n",
        "            optimal_batch_size = optimal_params['batch_size']\n",
        "            overlap_offset = optimal_params['overlap_offset']\n",
        "            overlap_core_length = optimal_params['overlap_core_length']\n",
        "\n",
        "            # Use optimized thresholds for effector annotation\n",
        "            final_peak_threshold = 0.6  # Optimized for sensitivity\n",
        "            final_edge_threshold = 0.1\n",
        "        else:\n",
        "            print(\"‚ùå Could not determine optimal parameters\")\n",
        "            return False\n",
        "    else:\n",
        "        # Manual mode - use user inputs\n",
        "        print(f\"\\n‚öôÔ∏è  Manual parameter mode selected\")\n",
        "\n",
        "        if subsequence_length == \"auto\":\n",
        "            # Use conservative auto settings even in manual mode\n",
        "            conservative_params, _ = get_conservative_parameters(lineage)\n",
        "            final_subsequence_length = conservative_params['subsequence_length']\n",
        "        elif subsequence_length == \"custom\":\n",
        "            final_subsequence_length = custom_subsequence_length\n",
        "        else:\n",
        "            final_subsequence_length = int(subsequence_length)\n",
        "\n",
        "        optimal_batch_size = int(batch_size) if batch_size != \"auto\" else 32\n",
        "\n",
        "        # Calculate overlap parameters\n",
        "        if use_overlap:\n",
        "            overlap_offset = final_subsequence_length // 2\n",
        "            overlap_core_length = int(final_subsequence_length * 0.75)\n",
        "        else:\n",
        "            overlap_offset = None\n",
        "            overlap_core_length = None\n",
        "\n",
        "        final_peak_threshold = peak_threshold\n",
        "        final_edge_threshold = edge_threshold\n",
        "\n",
        "        # Validate against N50 in manual mode\n",
        "        if genome_stats and final_subsequence_length > genome_stats['n50'] * 0.5:\n",
        "            print(f\"‚ö†Ô∏è  Warning: Subsequence length ({final_subsequence_length:,} bp) is large relative to N50 ({genome_stats['n50']/1000:.1f} kb)\")\n",
        "            print(\"   This may reduce prediction quality. Consider using Auto-Optimized mode.\")\n",
        "\n",
        "    # Create configuration dictionary\n",
        "    config = {\n",
        "        'job_name': job_name_clean,\n",
        "        'species_name': final_species,\n",
        "        'gff_feature_label': gff_feature_label,\n",
        "        'lineage': lineage,\n",
        "        'parameter_mode': parameter_mode,\n",
        "        'peak_threshold': final_peak_threshold,\n",
        "        'edge_threshold': final_edge_threshold,\n",
        "        'min_coding_length': min_coding_length,\n",
        "        'window_size': window_size,\n",
        "        'subsequence_length': final_subsequence_length,\n",
        "        'batch_size': optimal_batch_size,\n",
        "        'use_overlap': use_overlap,\n",
        "        'overlap_offset': overlap_offset,\n",
        "        'overlap_core_length': overlap_core_length,\n",
        "        'predict_phase': predict_phase,\n",
        "        'verbose_output': verbose_output,\n",
        "        'input_fasta': input_fasta_path,\n",
        "        'genome_stats': genome_stats\n",
        "    }\n",
        "\n",
        "    # Display configuration summary\n",
        "    print(\"\\n‚úÖ Configuration validated successfully!\")\n",
        "    print(f\"\\nüìã Job Configuration Summary:\")\n",
        "    print(f\"   üè∑Ô∏è  Job name: {config['job_name']}\")\n",
        "    print(f\"   üß¨ Species: {config['species_name']}\")\n",
        "    print(f\"   üåø Lineage model: {config['lineage']}\")\n",
        "    print(f\"   ‚öôÔ∏è  Parameter mode: {config['parameter_mode']}\")\n",
        "    print(f\"   üìä Peak threshold: {config['peak_threshold']} (effector-optimized)\")\n",
        "    print(f\"   üìè Subsequence length: {config['subsequence_length']:,} bp\")\n",
        "    print(f\"   üîÑ Use overlap: {config['use_overlap']}\")\n",
        "    print(f\"   üß© Batch size: {config['batch_size']}\")\n",
        "    print(f\"   üìñ Predict phases: {config['predict_phase']}\")\n",
        "\n",
        "    return config\n",
        "\n",
        "# Execute configuration validation\n",
        "prediction_config = validate_and_prepare_config()\n",
        "\n",
        "if prediction_config:\n",
        "    print(\"\\nüéâ Configuration ready for prediction!\")\n",
        "    if prediction_config['peak_threshold'] <= 0.6:\n",
        "        print(\"üéØ Using optimized peak threshold for effector annotation sensitivity\")\n",
        "    if prediction_config['parameter_mode'] == \"Auto-Optimized\":\n",
        "        print(\"‚ö° Using assembly-quality optimized parameters for best performance\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Configuration validation failed\")\n",
        "    print(\"‚ö†Ô∏è  Please check parameters and input file\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "Urs32gZqvpsC",
        "outputId": "e935f647-8a43-4ada-d0cb-8d666e845dfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ Parameter Optimization:\n",
            "   üìä Assembly quality: Good (N50: 152.3 kb)\n",
            "   üåø Lineage: fungi\n",
            "   ‚öôÔ∏è  Selected: Standard (optimized for good assembly)\n",
            "   üìè Subsequence length: 64,152 bp\n",
            "   üöÄ Expected speed improvement: 3.0x faster than minimum settings\n",
            "\n",
            "‚úÖ Configuration validated successfully!\n",
            "\n",
            "üìã Job Configuration Summary:\n",
            "   üè∑Ô∏è  Job name: helixer_annotation\n",
            "   üß¨ Species: GCA_0017028751\n",
            "   üåø Lineage model: fungi\n",
            "   ‚öôÔ∏è  Parameter mode: Auto-Optimized\n",
            "   üìä Peak threshold: 0.6 (effector-optimized)\n",
            "   üìè Subsequence length: 64,152 bp\n",
            "   üîÑ Use overlap: True\n",
            "   üß© Batch size: 128\n",
            "   üìñ Predict phases: True\n",
            "\n",
            "üéâ Configuration ready for prediction!\n",
            "üéØ Using optimized peak threshold for effector annotation sensitivity\n",
            "‚ö° Using assembly-quality optimized parameters for best performance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: Download Helixer Models\n",
        "#@title Download Lineage Models\n",
        "#@markdown Download the appropriate trained model for your selected lineage.\n",
        "\n",
        "import urllib.request\n",
        "import hashlib\n",
        "\n",
        "def download_helixer_models(target_lineage):\n",
        "    \"\"\"Download Helixer models for the specified lineage.\"\"\"\n",
        "\n",
        "    print(f\"üì• Downloading Helixer models for lineage: {target_lineage}\")\n",
        "\n",
        "    # Create models directory\n",
        "    models_dir = \"/content/helixer_models\"\n",
        "    lineage_dir = os.path.join(models_dir, target_lineage)\n",
        "    os.makedirs(lineage_dir, exist_ok=True)\n",
        "\n",
        "    # Model URLs and info\n",
        "    model_info = {\n",
        "        \"fungi\": {\n",
        "            \"filename\": \"fungi_v0.3_a_0100.h5\",\n",
        "            \"url\": \"https://zenodo.org/records/10836346/files/fungi_v0.3_a_0100.h5\"\n",
        "        },\n",
        "        \"land_plant\": {\n",
        "            \"filename\": \"land_plant_v0.3_a_0080.h5\",\n",
        "            \"url\": \"https://zenodo.org/records/10836346/files/land_plant_v0.3_a_0080.h5\"\n",
        "        },\n",
        "        \"vertebrate\": {\n",
        "            \"filename\": \"vertebrate_v0.3_m_0080.h5\",\n",
        "            \"url\": \"https://zenodo.org/records/10836346/files/vertebrate_v0.3_m_0080.h5\"\n",
        "        },\n",
        "        \"invertebrate\": {\n",
        "            \"filename\": \"invertebrate_v0.3_m_0100.h5\",\n",
        "            \"url\": \"https://zenodo.org/records/10836346/files/invertebrate_v0.3_m_0100.h5\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if target_lineage not in model_info:\n",
        "        print(f\"‚ùå Unknown lineage: {target_lineage}\")\n",
        "        return None\n",
        "\n",
        "    model_data = model_info[target_lineage]\n",
        "    model_path = os.path.join(lineage_dir, model_data[\"filename\"])\n",
        "\n",
        "    # Check if model already exists\n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"‚úÖ Model already exists: {model_path}\")\n",
        "        return model_path\n",
        "\n",
        "    try:\n",
        "        print(f\"üåê Downloading {model_data['filename']}...\")\n",
        "        print(\"‚è≥ This may take several minutes depending on connection speed...\")\n",
        "\n",
        "        def progress_hook(block_num, block_size, total_size):\n",
        "            if total_size > 0:\n",
        "                percent = min(100, (block_num * block_size * 100) // total_size)\n",
        "                if block_num % 100 == 0:  # Update every 100 blocks\n",
        "                    print(f\"   üìä Progress: {percent}% ({(block_num * block_size) // (1024*1024)} MB)\")\n",
        "\n",
        "        urllib.request.urlretrieve(\n",
        "            model_data[\"url\"],\n",
        "            model_path,\n",
        "            reporthook=progress_hook\n",
        "        )\n",
        "\n",
        "        # Verify download\n",
        "        if os.path.exists(model_path):\n",
        "            file_size = os.path.getsize(model_path)\n",
        "            print(f\"‚úÖ Download completed: {file_size // (1024*1024)} MB\")\n",
        "            return model_path\n",
        "        else:\n",
        "            print(\"‚ùå Download failed: file not found\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Download error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Download model for configured lineage\n",
        "if prediction_config:\n",
        "    model_path = download_helixer_models(prediction_config['lineage'])\n",
        "    if model_path:\n",
        "        prediction_config['model_path'] = model_path\n",
        "        print(f\"\\nüéâ Model ready: {os.path.basename(model_path)}\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Model download failed\")\n",
        "else:\n",
        "    print(\"‚ùå Please configure parameters first\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "KZSRdjf1ORdt",
        "outputId": "a077248e-b8df-48ef-80e7-5aa2e14b0b87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Downloading Helixer models for lineage: fungi\n",
            "üåê Downloading fungi_v0.3_a_0100.h5...\n",
            "‚è≥ This may take several minutes depending on connection speed...\n",
            "   üìä Progress: 0% (0 MB)\n",
            "   üìä Progress: 3% (0 MB)\n",
            "   üìä Progress: 6% (1 MB)\n",
            "   üìä Progress: 9% (2 MB)\n",
            "   üìä Progress: 13% (3 MB)\n",
            "   üìä Progress: 16% (3 MB)\n",
            "   üìä Progress: 19% (4 MB)\n",
            "   üìä Progress: 22% (5 MB)\n",
            "   üìä Progress: 26% (6 MB)\n",
            "   üìä Progress: 29% (7 MB)\n",
            "   üìä Progress: 32% (7 MB)\n",
            "   üìä Progress: 35% (8 MB)\n",
            "   üìä Progress: 39% (9 MB)\n",
            "   üìä Progress: 42% (10 MB)\n",
            "   üìä Progress: 45% (10 MB)\n",
            "   üìä Progress: 48% (11 MB)\n",
            "   üìä Progress: 52% (12 MB)\n",
            "   üìä Progress: 55% (13 MB)\n",
            "   üìä Progress: 58% (14 MB)\n",
            "   üìä Progress: 62% (14 MB)\n",
            "   üìä Progress: 65% (15 MB)\n",
            "   üìä Progress: 68% (16 MB)\n",
            "   üìä Progress: 71% (17 MB)\n",
            "   üìä Progress: 75% (17 MB)\n",
            "   üìä Progress: 78% (18 MB)\n",
            "   üìä Progress: 81% (19 MB)\n",
            "   üìä Progress: 84% (20 MB)\n",
            "   üìä Progress: 88% (21 MB)\n",
            "   üìä Progress: 91% (21 MB)\n",
            "   üìä Progress: 94% (22 MB)\n",
            "   üìä Progress: 97% (23 MB)\n",
            "‚úÖ Download completed: 23 MB\n",
            "\n",
            "üéâ Model ready: fungi_v0.3_a_0100.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: Run Helixer Prediction Pipeline with Results\n",
        "#@title Execute Helixer Gene Annotation and Download Results\n",
        "#@markdown Run the complete Helixer pipeline and immediately download/upload results to prevent data loss.\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "import glob\n",
        "from datetime import datetime\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "def run_helixer_pipeline_with_results(config):\n",
        "    \"\"\"Execute complete Helixer pipeline and handle results immediately.\"\"\"\n",
        "\n",
        "    if not config or 'model_path' not in config:\n",
        "        print(\"‚ùå Configuration or model not ready\")\n",
        "        return False\n",
        "\n",
        "    print(\"üöÄ Starting Helixer Gene Annotation Pipeline\")\n",
        "    print(f\"‚è∞ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # FIX: Get absolute path BEFORE changing directories\n",
        "    input_fasta = os.path.abspath(config['input_fasta'])\n",
        "    print(f\"üîç Input FASTA absolute path: {input_fasta}\")\n",
        "\n",
        "    # Verify file exists\n",
        "    if not os.path.exists(input_fasta):\n",
        "        print(f\"‚ùå Input FASTA file not found: {input_fasta}\")\n",
        "        return False\n",
        "\n",
        "    # Create working directory\n",
        "    work_dir = f\"/content/{config['job_name']}_helixer_work\"\n",
        "    os.makedirs(work_dir, exist_ok=True)\n",
        "    os.chdir(work_dir)\n",
        "\n",
        "    # File paths\n",
        "    h5_file = f\"{config['job_name']}.h5\"\n",
        "    predictions_file = \"predictions.h5\"\n",
        "    final_gff = f\"{config['job_name']}_helixer_annotation.gff3\"\n",
        "\n",
        "    pipeline_start = time.time()\n",
        "\n",
        "    try:\n",
        "        # Step 1: Convert FASTA to H5\n",
        "        print(\"\\nüîÑ Step 1/5: Converting FASTA to numerical matrices (fasta2h5)\")\n",
        "        print(f\"   üìÅ Input: {os.path.basename(input_fasta)}\")\n",
        "        print(f\"   üìÅ Output: {h5_file}\")\n",
        "        print(f\"   üìè Subsequence length: {config['subsequence_length']:,} bp\")\n",
        "\n",
        "        fasta2h5_cmd = [\n",
        "            \"python3\", \"/content/Helixer/fasta2h5.py\",\n",
        "            \"--species\", config['species_name'],\n",
        "            \"--h5-output-path\", h5_file,\n",
        "            \"--fasta-path\", input_fasta,\n",
        "            \"--subsequence-length\", str(config['subsequence_length'])\n",
        "        ]\n",
        "\n",
        "        if config['verbose_output']:\n",
        "            print(f\"   üíª Command: {' '.join(fasta2h5_cmd)}\")\n",
        "\n",
        "        step1_start = time.time()\n",
        "        result1 = subprocess.run(fasta2h5_cmd, capture_output=True, text=True)\n",
        "        step1_time = time.time() - step1_start\n",
        "\n",
        "        if result1.returncode != 0:\n",
        "            print(f\"‚ùå fasta2h5 failed: {result1.stderr}\")\n",
        "            return False\n",
        "\n",
        "        print(f\"‚úÖ Step 1 completed in {step1_time:.1f}s\")\n",
        "\n",
        "        # Step 2: Neural Network Prediction\n",
        "        print(\"\\nüß† Step 2/5: Neural network prediction (HybridModel)\")\n",
        "        print(f\"   ü§ñ Model: {os.path.basename(config['model_path'])}\")\n",
        "        print(f\"   üìä Batch size: {config['batch_size']}\")\n",
        "        print(f\"   ‚öôÔ∏è  Parameter mode: {config['parameter_mode']}\")\n",
        "\n",
        "        hybrid_cmd = [\n",
        "            \"python3\", \"/content/Helixer/helixer/prediction/HybridModel.py\",\n",
        "            \"--load-model-path\", config['model_path'],\n",
        "            \"--test-data\", h5_file,\n",
        "            \"--val-test-batch-size\", str(config['batch_size'])\n",
        "        ]\n",
        "\n",
        "        if config['use_overlap']:\n",
        "            hybrid_cmd.append(\"--overlap\")\n",
        "\n",
        "        if config['predict_phase']:\n",
        "            hybrid_cmd.append(\"--predict-phase\")\n",
        "\n",
        "        if config['verbose_output']:\n",
        "            hybrid_cmd.append(\"--verbose\")\n",
        "            print(f\"   üíª Command: {' '.join(hybrid_cmd)}\")\n",
        "\n",
        "        step2_start = time.time()\n",
        "        result2 = subprocess.run(hybrid_cmd, capture_output=True, text=True)\n",
        "        step2_time = time.time() - step2_start\n",
        "\n",
        "        if result2.returncode != 0:\n",
        "            print(f\"‚ùå HybridModel failed: {result2.stderr}\")\n",
        "            return False\n",
        "\n",
        "        print(f\"‚úÖ Step 2 completed in {step2_time:.1f}s\")\n",
        "\n",
        "        # Step 3: Post-processing to GFF3\n",
        "        print(\"\\nüìù Step 3/5: Post-processing to GFF3 (helixer_post_bin)\")\n",
        "        print(f\"   üéØ Peak threshold: {config['peak_threshold']} (effector-optimized)\")\n",
        "        print(f\"   üìè Min coding length: {config['min_coding_length']} bp\")\n",
        "\n",
        "        post_cmd = [\n",
        "            \"helixer_post_bin\",\n",
        "            h5_file,\n",
        "            predictions_file,\n",
        "            str(config['window_size']),\n",
        "            str(config['edge_threshold']),\n",
        "            str(config['peak_threshold']),\n",
        "            str(config['min_coding_length']),\n",
        "            final_gff\n",
        "        ]\n",
        "\n",
        "        if config['verbose_output']:\n",
        "            print(f\"   üíª Command: {' '.join(post_cmd)}\")\n",
        "\n",
        "        step3_start = time.time()\n",
        "        result3 = subprocess.run(post_cmd, capture_output=True, text=True)\n",
        "        step3_time = time.time() - step3_start\n",
        "\n",
        "        if result3.returncode != 0:\n",
        "            print(f\"‚ùå Post-processing failed: {result3.stderr}\")\n",
        "            return False\n",
        "\n",
        "        print(f\"‚úÖ Step 3 completed in {step3_time:.1f}s\")\n",
        "\n",
        "        # Validate output\n",
        "        if not os.path.exists(final_gff):\n",
        "            print(\"‚ùå Final GFF3 file not generated\")\n",
        "            return False\n",
        "\n",
        "        # Step 4: Extract sequences with gffread and create Geneious-compatible GFF\n",
        "        print(\"\\nüìã Step 4/5: Extracting sequences and creating Geneious-compatible GFF\")\n",
        "\n",
        "        # Extract proteins\n",
        "        proteins_file = f\"{config['job_name']}_proteins.fasta\"\n",
        "        gffread_cmd = [\"gffread\", \"-y\", proteins_file, \"-g\", input_fasta, final_gff]\n",
        "        result_proteins = subprocess.run(gffread_cmd, capture_output=True, text=True)\n",
        "        if result_proteins.returncode == 0:\n",
        "            print(f\"   ‚úÖ Proteins extracted: {proteins_file}\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  Protein extraction failed: {result_proteins.stderr}\")\n",
        "\n",
        "        # Extract transcripts\n",
        "        transcripts_file = f\"{config['job_name']}_transcripts.fasta\"\n",
        "        gffread_cmd = [\"gffread\", \"-w\", transcripts_file, \"-g\", input_fasta, final_gff]\n",
        "        result_transcripts = subprocess.run(gffread_cmd, capture_output=True, text=True)\n",
        "        if result_transcripts.returncode == 0:\n",
        "            print(f\"   ‚úÖ Transcripts extracted: {transcripts_file}\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  Transcript extraction failed: {result_transcripts.stderr}\")\n",
        "\n",
        "        # Extract CDS\n",
        "        cds_file = f\"{config['job_name']}_cds.fasta\"\n",
        "        gffread_cmd = [\"gffread\", \"-x\", cds_file, \"-g\", input_fasta, final_gff]\n",
        "        result_cds = subprocess.run(gffread_cmd, capture_output=True, text=True)\n",
        "        if result_cds.returncode == 0:\n",
        "            print(f\"   ‚úÖ CDS extracted: {cds_file}\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  CDS extraction failed: {result_cds.stderr}\")\n",
        "\n",
        "        # Extract gene sequences\n",
        "        genes_file = f\"{config['job_name']}_genes.fasta\"\n",
        "        gffread_cmd = [\"gffread\", \"-G\", genes_file, \"-g\", input_fasta, final_gff]\n",
        "        result_genes = subprocess.run(gffread_cmd, capture_output=True, text=True)\n",
        "        if result_genes.returncode == 0:\n",
        "            print(f\"   ‚úÖ Genes extracted: {genes_file}\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  Gene extraction failed: {result_genes.stderr}\")\n",
        "\n",
        "        # Create Geneious-compatible GFF\n",
        "        print(\"\\nüîß Step 5/5: Creating Geneious-compatible GFF\")\n",
        "        geneious_gff = f\"{config['job_name']}_geneious.gff3\"\n",
        "\n",
        "        with open(final_gff, 'r') as infile, open(geneious_gff, 'w') as outfile:\n",
        "            for line in infile:\n",
        "                if line.startswith('#'):\n",
        "                    outfile.write(line)\n",
        "                    continue\n",
        "\n",
        "                if line.strip():\n",
        "                    fields = line.strip().split('\\t')\n",
        "                    if len(fields) >= 9:\n",
        "                        # Clean up the attributes for Geneious compatibility\n",
        "                        attributes = fields[8]\n",
        "\n",
        "                        # Remove feature-specific numbering from IDs\n",
        "                        attributes = re.sub(r'\\.exon\\.\\d+', '', attributes)\n",
        "                        attributes = re.sub(r'\\.CDS\\.\\d+', '', attributes)\n",
        "                        attributes = re.sub(r'\\.five_prime_UTR\\.\\d+', '', attributes)\n",
        "                        attributes = re.sub(r'\\.three_prime_UTR\\.\\d+', '', attributes)\n",
        "\n",
        "                        # Remove frame info for CDS to make identical IDs\n",
        "                        if fields[2] == 'CDS':\n",
        "                            fields[7] = '.'  # Remove phase/frame\n",
        "\n",
        "                        fields[8] = attributes\n",
        "                        outfile.write('\\t'.join(fields) + '\\n')\n",
        "\n",
        "        print(f\"   ‚úÖ Geneious-compatible GFF created: {geneious_gff}\")\n",
        "\n",
        "        # Parse GFF3 for detailed statistics\n",
        "        print(\"\\nüìä Analyzing annotation results...\")\n",
        "        feature_counts = {}\n",
        "        total_cds_length = 0\n",
        "        gene_lengths = []\n",
        "        exon_counts = {}\n",
        "\n",
        "        with open(final_gff, 'r') as f:\n",
        "            current_gene_length = 0\n",
        "            current_gene_exons = 0\n",
        "\n",
        "            for line in f:\n",
        "                if line.startswith('#') or not line.strip():\n",
        "                    continue\n",
        "\n",
        "                fields = line.strip().split('\\t')\n",
        "                if len(fields) >= 9:\n",
        "                    feature_type = fields[2]\n",
        "                    start, end = int(fields[3]), int(fields[4])\n",
        "                    length = end - start + 1\n",
        "\n",
        "                    feature_counts[feature_type] = feature_counts.get(feature_type, 0) + 1\n",
        "\n",
        "                    if feature_type == 'gene':\n",
        "                        if current_gene_length > 0:\n",
        "                            gene_lengths.append(current_gene_length)\n",
        "                            exon_counts[current_gene_exons] = exon_counts.get(current_gene_exons, 0) + 1\n",
        "                        current_gene_length = length\n",
        "                        current_gene_exons = 0\n",
        "                    elif feature_type == 'exon':\n",
        "                        current_gene_exons += 1\n",
        "                    elif feature_type == 'CDS':\n",
        "                        total_cds_length += length\n",
        "\n",
        "            # Handle last gene\n",
        "            if current_gene_length > 0:\n",
        "                gene_lengths.append(current_gene_length)\n",
        "                exon_counts[current_gene_exons] = exon_counts.get(current_gene_exons, 0) + 1\n",
        "\n",
        "        pipeline_time = time.time() - pipeline_start\n",
        "\n",
        "        # Create comprehensive results package\n",
        "        print(\"\\nüì¶ Creating comprehensive results package...\")\n",
        "        results_zip = f\"/content/{config['job_name']}_helixer_results.zip\"\n",
        "\n",
        "        with zipfile.ZipFile(results_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            # Add GFF3 files\n",
        "            zipf.write(final_gff, os.path.basename(final_gff))\n",
        "            if os.path.exists(geneious_gff):\n",
        "                zipf.write(geneious_gff, os.path.basename(geneious_gff))\n",
        "\n",
        "            # Add extracted sequences\n",
        "            if os.path.exists(proteins_file):\n",
        "                zipf.write(proteins_file, os.path.basename(proteins_file))\n",
        "            if os.path.exists(transcripts_file):\n",
        "                zipf.write(transcripts_file, os.path.basename(transcripts_file))\n",
        "            if os.path.exists(cds_file):\n",
        "                zipf.write(cds_file, os.path.basename(cds_file))\n",
        "            if os.path.exists(genes_file):\n",
        "                zipf.write(genes_file, os.path.basename(genes_file))\n",
        "\n",
        "            # Create detailed summary file\n",
        "            summary_file = f\"{config['job_name']}_summary.txt\"\n",
        "            with open(summary_file, 'w') as f:\n",
        "                f.write(\"Helixer Gene Annotation Results\\n\")\n",
        "                f.write(\"=\" * 40 + \"\\n\\n\")\n",
        "\n",
        "                # Job information\n",
        "                f.write(\"JOB INFORMATION:\\n\")\n",
        "                f.write(f\"Job name: {config['job_name']}\\n\")\n",
        "                f.write(f\"Species: {config['species_name']}\\n\")\n",
        "                f.write(f\"Lineage model: {config['lineage']}\\n\")\n",
        "                f.write(f\"Parameter mode: {config['parameter_mode']}\\n\")\n",
        "                f.write(f\"Peak threshold: {config['peak_threshold']}\\n\")\n",
        "                f.write(f\"Edge threshold: {config['edge_threshold']}\\n\")\n",
        "                f.write(f\"Min coding length: {config['min_coding_length']} bp\\n\")\n",
        "                f.write(f\"Subsequence length: {config['subsequence_length']:,} bp\\n\")\n",
        "                f.write(f\"Processing time: {pipeline_time:.1f}s ({pipeline_time/60:.1f} minutes)\\n\")\n",
        "                f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "\n",
        "                # Assembly statistics\n",
        "                if config['genome_stats']:\n",
        "                    stats = config['genome_stats']\n",
        "                    f.write(\"ASSEMBLY STATISTICS:\\n\")\n",
        "                    f.write(f\"Assembly quality: {stats['quality_class']} ({stats['quality_desc']})\\n\")\n",
        "                    f.write(f\"Genome size: {stats['total_length'] / 1_000_000:.1f} Mb\\n\")\n",
        "                    f.write(f\"Number of sequences: {stats['sequence_count']:,}\\n\")\n",
        "                    f.write(f\"Sequence N50: {stats['n50'] / 1000:.1f} kb\\n\")\n",
        "                    f.write(f\"Sequence N90: {stats['n90'] / 1000:.1f} kb\\n\")\n",
        "                    f.write(f\"GC content: {stats['gc_percent']:.1f}%\\n\\n\")\n",
        "\n",
        "                # Feature statistics\n",
        "                f.write(\"ANNOTATION STATISTICS:\\n\")\n",
        "                for feature, count in sorted(feature_counts.items()):\n",
        "                    f.write(f\"{feature}: {count:,}\\n\")\n",
        "                f.write(f\"\\n\")\n",
        "\n",
        "                # Gene statistics\n",
        "                if gene_lengths:\n",
        "                    f.write(\"GENE LENGTH STATISTICS:\\n\")\n",
        "                    f.write(f\"Total genes: {len(gene_lengths):,}\\n\")\n",
        "                    f.write(f\"Average gene length: {sum(gene_lengths) / len(gene_lengths):,.0f} bp\\n\")\n",
        "                    f.write(f\"Shortest gene: {min(gene_lengths):,} bp\\n\")\n",
        "                    f.write(f\"Longest gene: {max(gene_lengths):,} bp\\n\")\n",
        "                    f.write(f\"Median gene length: {sorted(gene_lengths)[len(gene_lengths)//2]:,} bp\\n\")\n",
        "                    f.write(f\"Total coding length: {total_cds_length:,} bp\\n\\n\")\n",
        "\n",
        "                # Exon distribution\n",
        "                if exon_counts:\n",
        "                    f.write(\"EXON DISTRIBUTION:\\n\")\n",
        "                    for exon_count in sorted(exon_counts.keys()):\n",
        "                        if exon_count > 0:\n",
        "                            f.write(f\"Genes with {exon_count} exons: {exon_counts[exon_count]:,}\\n\")\n",
        "\n",
        "                f.write(f\"\\n\")\n",
        "                f.write(\"FILES INCLUDED:\\n\")\n",
        "                f.write(f\"‚Ä¢ {os.path.basename(final_gff)} - Standard Helixer GFF3 annotation\\n\")\n",
        "                f.write(f\"‚Ä¢ {os.path.basename(geneious_gff)} - Geneious-compatible GFF3\\n\")\n",
        "                f.write(f\"‚Ä¢ {os.path.basename(proteins_file)} - Protein sequences\\n\")\n",
        "                f.write(f\"‚Ä¢ {os.path.basename(transcripts_file)} - Transcript sequences\\n\")\n",
        "                f.write(f\"‚Ä¢ {os.path.basename(cds_file)} - CDS sequences\\n\")\n",
        "                f.write(f\"‚Ä¢ {os.path.basename(genes_file)} - Gene sequences\\n\\n\")\n",
        "\n",
        "                f.write(\"NEXT STEPS:\\n\")\n",
        "                f.write(f\"‚Ä¢ Validate with BUSCO: busco -i {os.path.basename(proteins_file)} -l {config['lineage']}_odb10 -m proteins\\n\")\n",
        "                f.write(\"‚Ä¢ Visualize in genome browsers (IGV, JBrowse, Geneious)\\n\")\n",
        "                f.write(\"‚Ä¢ Perform functional annotation (InterProScan, eggNOG-mapper)\\n\")\n",
        "                f.write(\"‚Ä¢ Compare with other gene predictors for consensus\\n\")\n",
        "\n",
        "            zipf.write(summary_file, os.path.basename(summary_file))\n",
        "\n",
        "            # Add configuration file\n",
        "            config_file = f\"{config['job_name']}_config.txt\"\n",
        "            with open(config_file, 'w') as f:\n",
        "                f.write(\"Helixer Configuration Used\\n\")\n",
        "                f.write(\"=\" * 30 + \"\\n\\n\")\n",
        "                for key, value in config.items():\n",
        "                    if key not in ['input_fasta', 'genome_stats']:  # Skip file path and complex objects\n",
        "                        f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "            zipf.write(config_file, os.path.basename(config_file))\n",
        "\n",
        "        file_size_kb = os.path.getsize(results_zip) / 1024\n",
        "        print(f\"‚úÖ Results package created: {os.path.basename(results_zip)} ({file_size_kb:.1f} KB)\")\n",
        "\n",
        "        # Display comprehensive results summary\n",
        "        print(\"\\nüéâ Helixer Pipeline Completed Successfully!\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"‚è∞ Total processing time: {pipeline_time:.1f}s ({pipeline_time/60:.1f} minutes)\")\n",
        "        if config['genome_stats']:\n",
        "            stats = config['genome_stats']\n",
        "            print(f\"üìä Assembly: {stats['quality_class']} quality ({stats['n50']/1000:.1f} kb N50)\")\n",
        "        print(f\"\\nüìä Annotation Results:\")\n",
        "        print(f\"   üß¨ Predicted genes: {feature_counts.get('gene', 0):,}\")\n",
        "        print(f\"   üî¨ CDS features: {feature_counts.get('CDS', 0):,}\")\n",
        "        print(f\"   üß™ Exon features: {feature_counts.get('exon', 0):,}\")\n",
        "\n",
        "        if gene_lengths:\n",
        "            avg_gene_length = sum(gene_lengths) / len(gene_lengths)\n",
        "            print(f\"   üìè Average gene length: {avg_gene_length:,.0f} bp\")\n",
        "            print(f\"   üìê Total coding length: {total_cds_length:,} bp\")\n",
        "\n",
        "        print(f\"\\nüìÅ Files generated:\")\n",
        "        print(f\"   ‚Ä¢ {os.path.basename(final_gff)} (standard annotation)\")\n",
        "        print(f\"   ‚Ä¢ {os.path.basename(geneious_gff)} (Geneious-compatible)\")\n",
        "        print(f\"   ‚Ä¢ {os.path.basename(proteins_file)} (protein sequences)\")\n",
        "        print(f\"   ‚Ä¢ {os.path.basename(transcripts_file)} (transcript sequences)\")\n",
        "        print(f\"   ‚Ä¢ {os.path.basename(cds_file)} (CDS sequences)\")\n",
        "        print(f\"   ‚Ä¢ {os.path.basename(genes_file)} (gene sequences)\")\n",
        "        print(f\"   ‚Ä¢ {os.path.basename(results_zip)} (complete package)\")\n",
        "\n",
        "        # Immediate upload to Google Drive\n",
        "        drive_url = None\n",
        "        if drive_service and helixer_folder_id:\n",
        "            print(f\"\\n‚òÅÔ∏è  Uploading to Google Drive...\")\n",
        "            drive_url = upload_to_drive(results_zip, config['job_name'], \"annotation\")\n",
        "            if drive_url:\n",
        "                print(f\"‚úÖ Google Drive upload successful!\")\n",
        "                print(f\"üîó Access link: {drive_url}\")\n",
        "\n",
        "        # Immediate local download\n",
        "        print(f\"\\n‚¨áÔ∏è  Initiating local download...\")\n",
        "        try:\n",
        "            files.download(results_zip)\n",
        "            print(\"‚úÖ Local download initiated\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Local download error: {e}\")\n",
        "            print(\"üìÅ File saved in Colab session - manually download if needed\")\n",
        "\n",
        "        # Cleanup intermediate files\n",
        "        print(f\"\\nüßπ Cleaning up intermediate files...\")\n",
        "        cleanup_files = [h5_file, predictions_file, summary_file, config_file]\n",
        "        for temp_file in cleanup_files:\n",
        "            if os.path.exists(temp_file):\n",
        "                os.remove(temp_file)\n",
        "                print(f\"   üóëÔ∏è  Removed: {temp_file}\")\n",
        "\n",
        "        print(f\"\\nüèÅ Annotation pipeline completed successfully!\")\n",
        "        print(f\"üìã Results summary:\")\n",
        "        print(f\"   ‚Ä¢ Standard GFF3 with {feature_counts.get('gene', 0):,} genes\")\n",
        "        print(f\"   ‚Ä¢ Geneious-compatible GFF3 for visualization\")\n",
        "        print(f\"   ‚Ä¢ Extracted protein, transcript, CDS, and gene sequences\")\n",
        "        print(f\"   ‚Ä¢ Comprehensive results package with statistics\")\n",
        "        print(f\"   ‚Ä¢ Google Drive backup: {output_folder_name}\")\n",
        "        print(f\"   ‚Ä¢ Local download initiated\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Pipeline error: {e}\")\n",
        "        return False\n",
        "\n",
        "    finally:\n",
        "        # Return to content directory\n",
        "        os.chdir(\"/content\")\n",
        "\n",
        "# Execute the complete pipeline with immediate results handling\n",
        "if prediction_config:\n",
        "    success = run_helixer_pipeline_with_results(prediction_config)\n",
        "    if success:\n",
        "        print(\"\\nüéä Gene annotation completed successfully!\")\n",
        "        print(\"üìã Your results are now available in:\")\n",
        "        print(\"   ‚Ä¢ Local download (started automatically)\")\n",
        "        print(f\"   ‚Ä¢ Google Drive: {output_folder_name} folder\")\n",
        "        print(\"\\nüí° Next steps:\")\n",
        "        print(\"   ‚Ä¢ Import Geneious-compatible GFF into genome viewers\")\n",
        "        print(\"   ‚Ä¢ Use extracted sequences for functional annotation\")\n",
        "        print(\"   ‚Ä¢ Validate annotations with BUSCO\")\n",
        "        print(\"   ‚Ä¢ Compare with other gene predictors\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Pipeline failed - check error messages above\")\n",
        "        print(\"üîß See troubleshooting guide below for common solutions\")\n",
        "else:\n",
        "    print(\"‚ùå Please configure parameters and download models first\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "id": "9o8mEotZvwYC",
        "outputId": "62f1a3d4-9ec3-4f4c-916e-4752db848f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Helixer Gene Annotation Pipeline\n",
            "‚è∞ Started at: 2025-06-25 16:28:04\n",
            "============================================================\n",
            "üîç Input FASTA absolute path: /content/GCA_001702875.1_Fol004_genomic.fasta\n",
            "\n",
            "üîÑ Step 1/5: Converting FASTA to numerical matrices (fasta2h5)\n",
            "   üìÅ Input: GCA_001702875.1_Fol004_genomic.fasta\n",
            "   üìÅ Output: helixer_annotation.h5\n",
            "   üìè Subsequence length: 64,152 bp\n",
            "   üíª Command: python3 /content/Helixer/fasta2h5.py --species GCA_0017028751 --h5-output-path helixer_annotation.h5 --fasta-path /content/GCA_001702875.1_Fol004_genomic.fasta --subsequence-length 64152\n",
            "‚úÖ Step 1 completed in 64.0s\n",
            "\n",
            "üß† Step 2/5: Neural network prediction (HybridModel)\n",
            "   ü§ñ Model: fungi_v0.3_a_0100.h5\n",
            "   üìä Batch size: 128\n",
            "   ‚öôÔ∏è  Parameter mode: Auto-Optimized\n",
            "   üíª Command: python3 /content/Helixer/helixer/prediction/HybridModel.py --load-model-path /content/helixer_models/fungi/fungi_v0.3_a_0100.h5 --test-data helixer_annotation.h5 --val-test-batch-size 128 --overlap --predict-phase --verbose\n",
            "‚úÖ Step 2 completed in 428.0s\n",
            "\n",
            "üìù Step 3/5: Post-processing to GFF3 (helixer_post_bin)\n",
            "   üéØ Peak threshold: 0.6 (effector-optimized)\n",
            "   üìè Min coding length: 60 bp\n",
            "   üíª Command: helixer_post_bin helixer_annotation.h5 predictions.h5 100 0.1 0.6 60 helixer_annotation_helixer_annotation.gff3\n",
            "‚úÖ Step 3 completed in 1259.9s\n",
            "\n",
            "üìã Step 4/5: Extracting sequences and creating Geneious-compatible GFF\n",
            "   ‚úÖ Proteins extracted: helixer_annotation_proteins.fasta\n",
            "   ‚úÖ Transcripts extracted: helixer_annotation_transcripts.fasta\n",
            "   ‚úÖ CDS extracted: helixer_annotation_cds.fasta\n",
            "   ‚ö†Ô∏è  Gene extraction failed: Error: cannot open input file helixer_annotation_genes.fasta!\n",
            "\n",
            "\n",
            "üîß Step 5/5: Creating Geneious-compatible GFF\n",
            "   ‚úÖ Geneious-compatible GFF created: helixer_annotation_geneious.gff3\n",
            "\n",
            "üìä Analyzing annotation results...\n",
            "\n",
            "üì¶ Creating comprehensive results package...\n",
            "‚úÖ Results package created: helixer_annotation_helixer_results.zip (27771.4 KB)\n",
            "\n",
            "üéâ Helixer Pipeline Completed Successfully!\n",
            "============================================================\n",
            "‚è∞ Total processing time: 1755.6s (29.3 minutes)\n",
            "üìä Assembly: Good quality (152.3 kb N50)\n",
            "\n",
            "üìä Annotation Results:\n",
            "   üß¨ Predicted genes: 20,545\n",
            "   üî¨ CDS features: 56,102\n",
            "   üß™ Exon features: 58,919\n",
            "   üìè Average gene length: 1,779 bp\n",
            "   üìê Total coding length: 26,070,646 bp\n",
            "\n",
            "üìÅ Files generated:\n",
            "   ‚Ä¢ helixer_annotation_helixer_annotation.gff3 (standard annotation)\n",
            "   ‚Ä¢ helixer_annotation_geneious.gff3 (Geneious-compatible)\n",
            "   ‚Ä¢ helixer_annotation_proteins.fasta (protein sequences)\n",
            "   ‚Ä¢ helixer_annotation_transcripts.fasta (transcript sequences)\n",
            "   ‚Ä¢ helixer_annotation_cds.fasta (CDS sequences)\n",
            "   ‚Ä¢ helixer_annotation_genes.fasta (gene sequences)\n",
            "   ‚Ä¢ helixer_annotation_helixer_results.zip (complete package)\n",
            "\n",
            "‚òÅÔ∏è  Uploading to Google Drive...\n",
            "‚òÅÔ∏è  Uploading helixer_annotation_helixer_results.zip to Google Drive...\n",
            "‚úÖ Upload complete: https://drive.google.com/file/d/1y_WLYkPTo0j6UA4eiy8OL3I3kngoDbUM/view\n",
            "‚úÖ Google Drive upload successful!\n",
            "üîó Access link: https://drive.google.com/file/d/1y_WLYkPTo0j6UA4eiy8OL3I3kngoDbUM/view\n",
            "\n",
            "‚¨áÔ∏è  Initiating local download...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f108042d-c29a-42e4-82d1-8534eb51cd63\", \"helixer_annotation_helixer_results.zip\", 28437866)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Local download initiated\n",
            "\n",
            "üßπ Cleaning up intermediate files...\n",
            "   üóëÔ∏è  Removed: helixer_annotation.h5\n",
            "   üóëÔ∏è  Removed: predictions.h5\n",
            "   üóëÔ∏è  Removed: helixer_annotation_summary.txt\n",
            "   üóëÔ∏è  Removed: helixer_annotation_config.txt\n",
            "\n",
            "üèÅ Annotation pipeline completed successfully!\n",
            "üìã Results summary:\n",
            "   ‚Ä¢ Standard GFF3 with 20,545 genes\n",
            "   ‚Ä¢ Geneious-compatible GFF3 for visualization\n",
            "   ‚Ä¢ Extracted protein, transcript, CDS, and gene sequences\n",
            "   ‚Ä¢ Comprehensive results package with statistics\n",
            "   ‚Ä¢ Google Drive backup: Helixer_Output\n",
            "   ‚Ä¢ Local download initiated\n",
            "\n",
            "üéä Gene annotation completed successfully!\n",
            "üìã Your results are now available in:\n",
            "   ‚Ä¢ Local download (started automatically)\n",
            "   ‚Ä¢ Google Drive: Helixer_Output folder\n",
            "\n",
            "üí° Next steps:\n",
            "   ‚Ä¢ Import Geneious-compatible GFF into genome viewers\n",
            "   ‚Ä¢ Use extracted sequences for functional annotation\n",
            "   ‚Ä¢ Validate annotations with BUSCO\n",
            "   ‚Ä¢ Compare with other gene predictors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîß Helixer Troubleshooting Guide & Parameter Reference\n",
        "\n",
        "## Assembly Quality & Parameter Guidelines\n",
        "\n",
        "### üèÜ **Excellent Assemblies (N50 > 1 Mb)**\n",
        "**Characteristics**: Chromosome-level, highly contiguous  \n",
        "**Optimized Settings**:\n",
        "- **Fungi**: 106,920 bp subsequences (5x faster)\n",
        "- **Plants**: 213,840 bp subsequences (3.3x faster)  \n",
        "- **Vertebrates/Invertebrates**: 320,760 bp subsequences (1.5x faster)\n",
        "- **Benefits**: Maximum speed, better gene context, improved accuracy\n",
        "\n",
        "### ü•à **Good Assemblies (N50 100kb-1Mb)**\n",
        "**Characteristics**: Scaffold-level, good contiguity  \n",
        "**Standard Settings**:\n",
        "- **Fungi**: 64,152 bp subsequences (3x faster)\n",
        "- **Plants**: 106,920 bp subsequences (1.7x faster)\n",
        "- **Vertebrates/Invertebrates**: 213,840 bp subsequences (standard)\n",
        "- **Benefits**: Balanced speed and safety\n",
        "\n",
        "### ü•â **Fair Assemblies (N50 10-100kb)**\n",
        "**Characteristics**: Contig-level, moderate fragmentation  \n",
        "**Conservative Settings**:\n",
        "- **All lineages**: Use GitHub defaults or smaller\n",
        "- **Safety**: Parameters guaranteed to work\n",
        "- **Trade-off**: Slower but reliable\n",
        "\n",
        "### ‚ö†Ô∏è **Poor Assemblies (N50 < 10kb)**\n",
        "**Characteristics**: Highly fragmented  \n",
        "**Minimal Settings**:\n",
        "- Use smallest possible subsequence lengths\n",
        "- Expect reduced annotation quality\n",
        "- Consider improving assembly first\n",
        "\n",
        "## Parameter Explanations (from GitHub)\n",
        "\n",
        "### **Core Prediction Parameters**\n",
        "\n",
        "**--peak-threshold** (0.1-1.0, default: 0.8)\n",
        "> Threshold specifies the minimum peak genic score required to accept the candidate region; the candidate region is accepted if it contains at least one window with a genic score above this threshold\n",
        "\n",
        "- **0.6**: Optimized for effector annotation (high sensitivity)\n",
        "- **0.8**: Standard annotation (balanced)\n",
        "- **0.9-0.95**: High precision annotation (low false positives)\n",
        "\n",
        "**--edge-threshold** (0.05-0.5, default: 0.1)  \n",
        "> Threshold specifies the genic score which defines the start/end boundaries of each candidate region within the sliding window\n",
        "\n",
        "**--min-coding-length** (default: 60)\n",
        "> Output is filtered to remove genes with a total coding length shorter than this value\n",
        "\n",
        "- **60 bp**: For effector genes (minimum)\n",
        "- **150+ bp**: For standard annotation quality\n",
        "\n",
        "**--window-size** (default: 100)\n",
        "> Width of the sliding window that is assessed for intergenic vs genic (UTR/Coding Sequence/Intron) content\n",
        "\n",
        "### **Performance Parameters**\n",
        "\n",
        "**--subsequence-length**\n",
        "> How to slice the genomic sequence. Set moderately longer than length of typical genic loci. Must be evenly divisible by the timestep width of the used model, which is typically 9.\n",
        "\n",
        "- **Critical**: Must be < 50% of your assembly N50\n",
        "- **Larger values**: Faster processing, better gene context\n",
        "- **GitHub defaults**: vertebrate: 213840, land_plant: 64152, fungi: 21384, invertebrate: 213840\n",
        "\n",
        "**--batch-size** (default: 32)\n",
        "> The batch size for the raw predictions in TensorFlow. Should be as large as possible on your GPU to save prediction time.\n",
        "\n",
        "**--overlap** / --no-overlap**\n",
        "> Switches off the overlapping after predictions are made. Overlap will improve prediction quality at subsequence ends by creating and overlapping sliding-window predictions.\n",
        "\n",
        "- **Benefits**: Better accuracy at sequence boundaries\n",
        "- **Cost**: ~2x processing time\n",
        "- **Recommended**: True for final annotations\n",
        "\n",
        "**--predict-phase**\n",
        "> Add this to also predict phases for CDS (recommended); format: [None, 0, 1, 2]; 'None' is used for non-CDS regions, within CDS regions 0, 1, 2 correspond to phase\n",
        "\n",
        "## Common Issues and Solutions\n",
        "\n",
        "### üö´ **GPU Out of Memory**\n",
        "**Problem**: CUDA out of memory during prediction  \n",
        "**Solutions**:\n",
        "- Reduce batch size (32 ‚Üí 16 ‚Üí 8)\n",
        "- Use smaller subsequence length\n",
        "- Switch to Conservative mode\n",
        "- Disable overlap temporarily\n",
        "\n",
        "### üìÅ **File Format Issues**  \n",
        "**Problem**: FASTA not recognized or parsing errors  \n",
        "**Solutions**:\n",
        "- Ensure headers start with '>'\n",
        "- Check for unusual characters\n",
        "- Verify file not corrupted\n",
        "- Try manual decompression\n",
        "\n",
        "### üß¨ **Poor Gene Predictions**\n",
        "**Problem**: Too many/few genes predicted  \n",
        "**Solutions**:\n",
        "- **Too many**: Increase peak-threshold (0.8-0.95)\n",
        "- **Too few**: Decrease peak-threshold (0.6), reduce min-coding-length\n",
        "- **Wrong lineage**: Verify model selection\n",
        "- **Assembly quality**: Check if N50 is very low\n",
        "\n",
        "### ‚ö° **Slow Performance**\n",
        "**Problem**: Prediction taking too long  \n",
        "**Solutions**:\n",
        "- Use Auto-Optimized mode\n",
        "- Increase batch size if GPU memory allows\n",
        "- Verify GPU is being used\n",
        "- Use larger subsequence lengths for good assemblies\n",
        "\n",
        "### üîß **Parameter Safety Violations**\n",
        "**Problem**: Subsequence length warnings  \n",
        "**Solutions**:\n",
        "- Always use Auto-Optimized mode for safety\n",
        "- Manual mode: ensure subsequence_length < 50% of N50\n",
        "- Consider improving assembly quality\n",
        "- Use Conservative mode for problematic assemblies\n",
        "\n",
        "## Expected Performance by Assembly Quality\n",
        "\n",
        "### üöÄ **Speed Improvements**\n",
        "| Assembly Quality | Subsequence Length | Speed Gain | Safety |\n",
        "|-----------------|-------------------|------------|---------|\n",
        "| **Excellent** | Up to 320kb | 1.5-5x faster | ‚úÖ Safe |\n",
        "| **Good** | Up to 213kb | 1.7-3x faster | ‚úÖ Safe |  \n",
        "| **Fair** | Up to 106kb | 1-2x faster | ‚ö†Ô∏è Check N50 |\n",
        "| **Poor** | Conservative | Baseline | ‚úÖ Safe |\n",
        "\n",
        "### üìä **BUSCO Validation Expectations**\n",
        "- **Excellent assemblies + Optimized params**: >95% complete\n",
        "- **Good assemblies + Standard params**: 90-95% complete\n",
        "- **Fair assemblies + Conservative params**: 85-90% complete\n",
        "- **Poor assemblies**: <85% complete (assembly limitation)\n",
        "\n",
        "## File Usage Guide\n",
        "\n",
        "### üìÅ **Output Files Explained**\n",
        "- **`*_annotation.gff3`**: Standard GFF3 for most tools\n",
        "- **`*_geneious.gff3`**: Optimized for Geneious/genome viewers\n",
        "- **`*_proteins.fasta`**: For BUSCO, functional annotation\n",
        "- **`*_transcripts.fasta`**: For RNA-seq analysis\n",
        "- **`*_cds.fasta`**: For phylogenetics, codon usage\n",
        "- **`*_genes.fasta`**: For promoter analysis\n",
        "- **`*_summary.txt`**: Detailed statistics and metrics\n",
        "\n",
        "### üîç **Genome Viewer Compatibility**\n",
        "- **Geneious Prime**: Use `*_geneious.gff3`\n",
        "- **IGV**: Use standard `*_annotation.gff3`\n",
        "- **JBrowse**: Use standard `*_annotation.gff3`\n",
        "- **UCSC Genome Browser**: May need format conversion\n",
        "\n",
        "## Additional Resources\n",
        "\n",
        "- **üìö GitHub**: https://github.com/weberlab-hhu/Helixer\n",
        "- **üìÑ Paper**: https://doi.org/10.1101/2023.02.06.527280\n",
        "- **üß¨ BUSCO**: https://busco.ezlab.org/\n",
        "- **üîß gffread**: https://github.com/gpertea/gffread\n",
        "- **üìñ Parameter docs**: Full parameter list in GitHub README\n",
        "\n",
        "## Getting Help\n",
        "\n",
        "1. **Check assembly statistics** - Most issues relate to poor assembly quality\n",
        "2. **Use Auto-Optimized mode** - Prevents most parameter problems\n",
        "3. **Validate with BUSCO** - Quantifies annotation completeness\n",
        "4. **Compare lineage models** - Try different models if results poor\n",
        "5. **Report issues** with assembly stats and error logs"
      ],
      "metadata": {
        "id": "wUSkb0e_xuRK"
      }
    }
  ]
}